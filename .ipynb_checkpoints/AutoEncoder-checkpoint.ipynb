{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 自编码器\n",
    "\n",
    "AutoEncoder\n",
    "\n",
    "徐静\n",
    "\n",
    "版权所有，转载请注明出处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "我们后期的重点仍然放在深度学习，文本挖掘，语音识别：\n",
    "\n",
    "深度学习部分：\n",
    "\n",
    "+ 自编码器\n",
    "\n",
    "+ 玻尔兹曼机、受限制玻尔兹曼机\n",
    "\n",
    "+ 深度玻尔兹曼机、实值上的受限制玻尔兹曼机，卷积玻尔兹曼机，用于结构化或序列输出的玻尔兹曼机，其他玻尔兹曼机\n",
    "\n",
    "+ 策略网络，估值网络\n",
    "\n",
    "+ 生成对抗网络（GAN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "文本挖掘\n",
    "\n",
    "在18年的1月份主要系统培训学习NLP的相关内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "语音识别\n",
    "\n",
    "后期也会加入语音识别的培训"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 现在还是回到自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**自编码器**（autoencoder）是神经网络的一种，经过训练后能尝试将输入复制到输出。 最早由Rumelhart在1986年提出，可以用来对高维数据进行降维。2006年，Hinton通过改进自编码器的学习算法，提出了深层自编码器的概念，深层自编码器主要用于完成数据转换的学习任务，在本质上是一种无监督学习的非线性特征提取模型。其学习算法具有典型性，由无监督预训练和有监督调优两个阶段构成，是许多深度学习算法的思想基础。本部slides介绍自编码器的标准模型，学习算法和相关变种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 自编码器的标准模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "标准的自编码器是一个关于中间层具有结构对称性的多层前馈网络，他的期望输出与输入相同，可以用来学习恒等映射并抽取无监督特征，如下图所示：一个单隐层的自编码器的例子。\n",
    "\n",
    "![](pic/output1.png)\n",
    "\n",
    "**图1：标准自编码器结构图**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "其中只有一个隐含层用于对输入进行编码，并通过解码在输出对输入进行重构。从根本上说，一个自编码器可以从输入生成一种隐含层表示，并从这种隐含层表示重构与输入尽可能接近的输出。输入层到中间层的部分称为编码器（encoder），而从中间层到输出层的部分称为解码器（decoder）。\n",
    "\n",
    "![](pic/output2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 自编码器的学习算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "作为一种特殊的多层感知机，从理论上来说自编码器可用过反向传播算法（既BP算法）来学习权重和偏置。不过由于局部极小的问题额存在，一个深层自编码器如果直接使用反向传播算法学习权重和偏置，得到的结果常常是不稳定的，也就是说，不同的初始化可能产生非常不同的结果。更有甚者，学习的过程常常收敛比较慢，甚至根本不收敛，在时间上很难得到令人满意的性能。\n",
    "\n",
    "一种有效的解决办法是采用两阶段训练方法，其中包含无监督训练和有监督调优两个阶段，说明如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**阶段1: 无监督训练**\n",
    "\n",
    "1. 用接近0的随机数初始化网络参数（$W^i$,$b^i$）\n",
    "\n",
    "2. 使用CD-k算法训练第1个RBM，该RBM的可视层为$x$，隐含层为$h_1$\n",
    "\n",
    "3. 对$1<i\\leq r$，把$h_{i-1}$作为第$i$个RBM的可视层，把$h_i$作为第$i$个RBM的隐层，使用CD-k算法逐层训练RBM\n",
    "\n",
    "4. 反向堆叠RBM，初始化$r+1$到$2r$层的自编码器参数\n",
    "\n",
    "**阶段2：有监督调优**\n",
    "\n",
    "  5.使用有监督算法(如反向传播)对网络参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 自编码器的变种模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "线性自编码器是自编码器的一种特例，也是最简单的变种，其中采用恒等函数作为激活函数。在目标函数取为平方重建误差时，线性自编码器从产生相同子空间的意义上等价于PCA。标准自编码器是PCA的一种非线性推广，其中的激活函数是非线性的，通过学习和训练可以获得更强大的特征提取能力。\n",
    "\n",
    "自编码器还有其他许多变种：特别的是有某些正则化机制的变种，如稀疏自编码器（Sparse AutoEncoder SAE）,降噪自编码器（Denoising AutoEncoder DAE）,收缩自编码器（CAE）,预测稀疏分解自编码器(predictive sparse decomposition AE,PSD AE),平滑自编码器(SmAE),卷积自编码器(CoAE)和反传无关自编码器(BFAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 稀疏自编码器（SAE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "稀疏自编码器在模型优化中增加了对隐含神经元激活的稀疏性约束，以使大多数隐含神经元都处于非激活状态。它的优化目标函数是平方重构误差与某个稀疏正则项之和，具体形式如下：\n",
    "\n",
    "$$L_{sparse} = L_N + \\beta \\sum_{j}KL(\\rho||\\hat{\\rho_j}) = \\frac{1}{2}\\sum_{l=1}^{N}\\sum_{j=1}^{c}(o_j^l-y_j^l)^2 + \\beta\\sum_{j}KL(\\rho||\\hat{\\rho_j})$$\n",
    "\n",
    "其中$\\rho$是稀疏性参数，通常取为接近于零的正实数，$\\hat{\\rho_j}$表示第$j$个隐含神经元的平均激活值\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 降噪自编码器（DAE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "降噪自编码器是一种通过特殊方式训练得到的自编码器，具体的做法是，在输入数据中增加一定的噪声，对自编码器进行训练，使其产生抗噪能力，从而产生更加稳健的数据重构效果。假设$x$为无噪声的原始输入，降噪自编码器首先利用一个随机映射$\\hat{x}\\sim q_D(\\hat{x}|x)$,把$x$腐蚀为部分受损的版本$\\hat{x}$，然后把$\\hat{x}$当做噪声的腐蚀输入，把$x$当做输出，对自编码器进行学习训练\n",
    "\n",
    "与标准的自编码器相比，降噪自编码器的主要不同在于输出$o$是关于$\\hat{x}$而不是关于$x$的函数，而且自编码器必须学会提取输入分布的结构，以最优的抵消腐蚀加噪过程的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 收缩自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "收缩自编码器和降噪自编码器具有类似动机，目的是为了学习数据的稳健表示，与降噪自编码器不同的是，收缩自编码器在平方重建误差的基础上增加了一个分析性的收缩惩罚项，这个惩罚项就是编码的雅克比矩阵的弗罗贝尼乌斯范数（Frobenius norm）(元素平方之和)，用于惩罚所学特征对微小输入变化的敏感性。用$J(x^l) = \\frac{\\partial{\\sigma_h(x^l)}}{\\partial{x}}$表示隐含层关于输入的雅克比矩阵，那么收缩自编码器优化的目标函数可以描述为\n",
    "\n",
    "$$L_{CAE} = L_N + \\lambda\\sum_{l=1}^{N}||J_f(x^l)||_F^2 = \\frac{1}{2}\\sum_{l=1}^{N}\\sum_{j=1}^{c}(o_j^l-y_j^l)+\\lambda\\sum_{l=1}{N}||J_f(x_l)||_F^2$$\n",
    "\n",
    "应该指出的是：收缩自编码器有一个潜在的缺点，它的分析惩罚项至多只能对输入的微小变化产生鲁棒性，为了修复这个缺点，一个可行的办法是采用‘CAE+H’模型，其所有高阶导数都按照某种有效的随机方法惩罚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 预测稀疏分解自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "预测稀疏分解自编码器在隐含层采用稀疏编码正则化，他有两个又是：一个是可以学到一组超完备的线性基，二个是能够产生一个光滑的和容易计算的逼近模型，用于预测最优的系数表示，其优化目标函数可以表示为：\n",
    "\n",
    "$$L_{PSD} = \\sum_{l=1}{N}\\lambda ||h^l||_1 + ||x_l-Wh_l||^2 + ||h_l-f(x^l)||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 平滑自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "对每一个输入样本，平滑自编码器要重建的是其临近点，而不是像标准自编码器那样重建其自身。平滑自编码器学到的表示在局部邻域中是一致的，并且对输入的微小变化是鲁棒的，甚至随着输入样本在流行上的变化是平滑的。平滑自编码器的目标函数定义为：\n",
    "\n",
    "$$L_{SmAE} = \\sum_{l=1}^{N}\\sum_{k=1}^{N_l}w(x^k,x^l)L(x^k,y^l)+\\beta\\sum_{j}KL(\\rho|\\hat{\\rho_j}))$$\n",
    "\n",
    "其中$N_l$是样本$x^l$的目标近邻点的个数，$w(x^k,x^l)$是定义的核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 卷积自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "卷积自编码器是Masci等人提出的，目的是利用二维图像中的结构信息。与标准自编码器的不同之处在于，卷积自编码器在输入的所有位置都共享权值，以保持二维空间的局部性。而且它基于隐含编码的基本图像块的线性组合重构输入，对于单通道的输入$x$,他的计算过程为：\n",
    "\n",
    "$$ \\left\\{\n",
    "\\begin{aligned}\n",
    "h^k& = \\sigma(x*w^k+b^k) \\\\\n",
    "o & = \\sigma(\\sum_{k \\in H}h^k * \\hat{w^k}+c)\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "这里的$h^k$表示第$k$个卷积面，$o$表示重构结果，$*$表示卷积操作，$w^k$,$b^k$表示第$k$个权重和偏置，$H$表示卷积面的指标集 $\\hat{w^k}$表示权重$w^k$进行两个维数方向翻转后得到的权值，$c$表示输入通道的偏置\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 反传无关自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "反传无关自编码器是Lee等人提出的，目的是不用反向传播算法训练性能良好的自编码器。基本的反传无关自编码器类似于基本的再循环过程。不同之处在于，反传无关自编码器可以使用非线性的编码器和解码器，以及任意的重构损失函数，甚至还可以像降噪自编码器那样注入噪声。反传无关自编码器的计算和优化过程如下：\n",
    "\n",
    "$$\n",
    " \\left\\{\n",
    "\\begin{aligned}\n",
    "h& = f(x)\\\\\n",
    "\\hat{x} & =  g(corrupt(h))\\\\\n",
    "\\hat{h} & =  f(corrupt(\\hat{x}))\\\\\n",
    "L_{BFAE} & = loss(g(corrupt(h),x) + loss(f(corrupt(\\hat{x})，h))\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "其中$f(.)$和$g(.)$是线性或非线性的激活函数，$corrupt(.)$表示注入噪声，$loss(.,.)$是所选的重构损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "最后，如果读者有兴趣或精力，还可以探讨另外的自编码器变种，包括分层稀疏自编码器，变换自编码器，门控自编码器，自适用降噪自编码器，渐进自编码器，变分自编码器，自编码器树，率失真自编码器以及k-稀疏自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow实现自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "step1: 导入必要的模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "step2:定义参数初始化方法xaiver initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def xaiver_init(fan_in,fan_out,constant=1):\n",
    "    low = -constant * np.sqrt(6.0/(fan_in + fan_out))\n",
    "    hifh = constant * np.sqrt(6.0/(fin_in + fan_out))\n",
    "    return tf.random_uniform((fan_in,fan_out),minval = low,maxval = high,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "step3:定义自编码器的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditivateGaussianNoiseAutoencoder(object):\n",
    "    '''\n",
    "    定义网络结构\n",
    "    '''\n",
    "    def __init__(self,n_input,n_hidden,transfer_function=tf.nn.softplus,optimizer=tf.train.AdamOptimizer(),scale=0.1):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = b_hidden\n",
    "        self.transfer = transfer_function\n",
    "        self.scale = tf.placeholder(tf.float32)\n",
    "        self.training_scale = scale\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32,[None,self.n_input])\n",
    "        self.hidden = self.transder(tf.add(tf.matmul(self.x + scale*tf.random_normal((n_input,)),self.weights['w1']),self.weights['b1']))\n",
    "        self.reconstruction = tf.add(tf.matmul(self.hidden,self.weights['w2']),self.weights['b2'])\n",
    "        \n",
    "        self.cost = 0.5*tf.reduce_sum(tf.pow(tf.substract(self.reconstruction,self.x),2.0))\n",
    "        self.optimizer = optimizer.minimize(self.cost)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        '''\n",
    "        定义初始化参数\n",
    "        '''\n",
    "        def _initialize_weights(self):\n",
    "            all_weights = dict()\n",
    "            all_weights['w1'] = tf.Variable(xavier_init(self.n_input,self.n_hidden))\n",
    "            all_weights['b1'] = tf.Variable(tf.zeros([self.n_hidden],dtype=tf.float32))\n",
    "            all_weights['w2'] = tf.variable(tf.zeros([self.n_hidden,self.n_input],dtype=tf.float32))\n",
    "            all_weights['b2'] = tf.variable(tf.zeros([self.n_input],dtype=tf.float32))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "11.988636px",
    "width": "1.988636px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
